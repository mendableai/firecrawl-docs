---
title: 'Crawl'
description: 'Firecrawl can recursively search through a urls subdomains, and gather the content'
og:title: "Crawl | Firecrawl"
og:description: "Firecrawl can recursively search through a urls subdomains, and gather the content"
---

import InstallationPython from "/snippets/v1/installation/python.mdx";
import InstallationNode from "/snippets/v1/installation/js.mdx";
import InstallationGo from "/snippets/v1/installation/go.mdx";
import InstallationRust from "/snippets/v1/installation/rust.mdx";
import CrawlPython from "/snippets/v1/crawl/base/python.mdx";
import CrawlNode from "/snippets/v1/crawl/base/js.mdx";
import CrawlGo from "/snippets/v1/crawl/base/go.mdx";
import CrawlRust from "/snippets/v1/crawl/base/rust.mdx";
import CrawlCURL from "/snippets/v1/crawl/base/curl.mdx";
import AsyncCrawlOutput from "/snippets/v1/crawl-async/base/output.mdx";
import CheckCrawlJobPython from "/snippets/v1/crawl-status/short/python.mdx";
import CheckCrawlJobNode from "/snippets/v1/crawl-status/short/js.mdx";
import CheckCrawlJobGo from "/snippets/v1/crawl-status/short/go.mdx";
import CheckCrawlJobRust from "/snippets/v1/crawl-status/short/rust.mdx";
import CheckCrawlJobCURL from "/snippets/v1/crawl-status/short/curl.mdx";
import CheckCrawlJobOutputScraping from "/snippets/v1/crawl-status/base/output-scraping.mdx";
import CheckCrawlJobOutputCompleted from "/snippets/v1/crawl-status/base/output-completed.mdx";
import CrawlWebSocketPython from "/snippets/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNode from "/snippets/v1/crawl-websocket/base/js.mdx";
import CrawlWebhookCURL from "/snippets/v1/crawl-webhook/base/curl.mdx";
import PythonCrawlExample from "/snippets/v1/crawl/sdk-example/python.mdx";
import NodeCrawlExample from "/snippets/v1/crawl/sdk-example/js.mdx";
import PythonCrawlExampleResponse from "/snippets/v1/crawl/sdk-example/python-response.mdx";
import NodeCrawlExampleResponse from "/snippets/v1/crawl/sdk-example/js-response.mdx";
import FastCrawlPython from "/snippets/v1/crawl/fast/python.mdx";
import FastCrawlNode from "/snippets/v1/crawl/fast/js.mdx";
import FastCrawlGo from "/snippets/v1/crawl/fast/go.mdx";
import FastCrawlRust from "/snippets/v1/crawl/fast/rust.mdx";
import FastCrawlCURL from "/snippets/v1/crawl/fast/curl.mdx";

Firecrawl efficiently crawls websites to extract comprehensive data while bypassing blockers. The process:

1. **URL Analysis:** Scans sitemap and crawls website to identify links
2. **Traversal:** Recursively follows links to find all subpages
3. **Scraping:** Extracts content from each page, handling JS and rate limits
4. **Output:** Converts data to clean markdown or structured format

This ensures thorough data collection from any starting URL.


## Crawling


### /crawl endpoint

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.


<Warning>By default - Crawl will ignore sublinks of a page if they aren't children of the url you provide. So, the website.com/other-parent/blog-1 wouldn't be returned if you crawled website.com/blogs/. If you want website.com/other-parent/blog-1, use the `crawlEntireDomain` parameter. To crawl subdomains like blog.website.com when crawling website.com, use the `allowSubdomains` parameter.</Warning>


### Installation 

<CodeGroup>

<InstallationPython />
<InstallationNode />
<InstallationGo />
<InstallationRust />

</CodeGroup>

### Usage

<CodeGroup>

<CrawlPython />
<CrawlNode />
<CrawlGo />
<CrawlRust />
<CrawlCURL />

</CodeGroup>

### API Response

If you're using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.

<Note>If you're using the SDK, check the SDK response section [below](#sdk-response).</Note>

<AsyncCrawlOutput />

### Check Crawl Job

Used to check the status of a crawl job and get its result.

<Note>This endpoint only works for crawls that are in progress or crawls that have completed recently. </Note>


<CodeGroup>

<CheckCrawlJobPython />
<CheckCrawlJobNode />
<CheckCrawlJobGo />
<CheckCrawlJobRust />
<CheckCrawlJobCURL />

</CodeGroup>

#### Response Handling

The response varies based on the crawl's status.

For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.

The skip parameter sets the maximum number of results returned for each chunk of results returned. 

<Info>
The skip and next parameter are only relavent when hitting the api directly. If you're using the SDK, we handle this for you and will return all the results at once.
</Info>


<CodeGroup>
  <CheckCrawlJobOutputScraping />
  <CheckCrawlJobOutputCompleted />
</CodeGroup>

### SDK Response

The SDK provides two ways to crawl URLs:

1. **Synchronous Crawling** (`crawl_url`/`crawlUrl`):
   - Waits for the crawl to complete and returns the full response
   - Handles pagination automatically
   - Recommended for most use cases

<CodeGroup>
  <PythonCrawlExample />
  <NodeCrawlExample />
</CodeGroup>

The response includes the crawl status and all scraped data:

<CodeGroup>
  <PythonCrawlExampleResponse />
  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Asynchronous Crawling** (`async_crawl_url`/`asyncCrawlUrl`):
   - Returns immediately with a crawl ID
   - Allows manual status checking
   - Useful for long-running crawls or custom polling logic

<CodeGroup>
  <AsyncCrawlPython />
  <AsyncCrawlNode />
</CodeGroup>

## Faster Crawling

Speed up your crawls by 500% when you don't need the freshest data. Add `maxAge` to your `scrapeOptions` to use cached page data when available.

<CodeGroup>
  <FastCrawlPython />
  <FastCrawlNode />
  <FastCrawlGo />
  <FastCrawlRust />
  <FastCrawlCURL />
</CodeGroup>

**How it works:**
- Each page in your crawl checks if we have cached data newer than `maxAge`
- If yes, returns instantly from cache (500% faster)
- If no, scrapes the page fresh and caches the result
- Perfect for crawling documentation sites, product catalogs, or other relatively static content

For more details on `maxAge` usage, see the [Faster Scraping](/features/fast-scraping) documentation.

## Crawl WebSocket

Firecrawl's WebSocket-based method, `Crawl URL and Watch`, enables real-time data extraction and monitoring. Start a crawl with a URL and customize it with options like page limits, allowed domains, and output formats, ideal for immediate data processing needs.

<CodeGroup>
  <CrawlWebSocketPython />
  <CrawlWebSocketNode />
</CodeGroup>

## Crawl Webhook

You can configure webhooks to receive real-time notifications as your crawl progresses. This allows you to process pages as they're scraped instead of waiting for the entire crawl to complete.

<CrawlWebhookCURL />

For comprehensive webhook documentation including event types, payload structure, and implementation examples, see the [Webhooks documentation](/features/webhooks).

### Quick Reference

**Event Types:**
- `crawl.started` - When the crawl begins
- `crawl.page` - For each page successfully scraped  
- `crawl.completed` - When the crawl finishes
- `crawl.failed` - If the crawl encounters an error

**Basic Payload:**
```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Page data for 'page' events
  "metadata": {}, // Your custom metadata
  "error": null
}
```

<Note>
For detailed webhook configuration, security best practices, and troubleshooting, visit the [Webhooks documentation](/features/webhooks).
</Note>

